{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OverfitSurvivor/code/blob/main/drone_DAE_plz_0416.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0A5zWRFi_QR",
        "outputId": "ae6d4d9f-55e4-4ed9-b954-9ebd84ddf8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RzgW8NHycMV",
        "outputId": "bc330d12-3b24-4895-ba62-9f864f6b22bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_msssim\n",
            "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch_msssim) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->pytorch_msssim)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch_msssim) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch_msssim) (3.0.2)\n",
            "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch_msssim\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch_msssim-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_msssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaVeakiafwSP",
        "outputId": "ad7e4d90-d83f-4523-9f01-ba09e91ef7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 해제 완료: /content/ICSV31AIChallengeDataset\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/ICSV31AIChallengeDataset.zip\"  # 업로드한 ZIP 파일 경로\n",
        "extract_path = \"/content/ICSV31AIChallengeDataset\"  # 압축을 풀 폴더 경로\n",
        "\n",
        "# 폴더가 없으면 생성\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# 압축 해제\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"압축 해제 완료:\", extract_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-K2Pl0PbbQ"
      },
      "source": [
        "## 모듈 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AcxPt6b6AAZy"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import argparse\n",
        "import os\n",
        "from typing import Any, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "421dhcInPX1n"
      },
      "source": [
        "## 라벨링 및 데이터셋 로더\n",
        "+ global scaling STFT 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bHs-4FinYIFu"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "# 1. Utils\n",
        "#######################\n",
        "def read_csv(file_path: str) -> List:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        return list(reader)\n",
        "\n",
        "def save_csv(save_data: List[Any], save_file_path: str) -> None:\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "def get_anomaly_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    train_mode = file_name.split(\"_\")[0]\n",
        "    if train_mode == \"test\":\n",
        "        return -1\n",
        "    elif \"normal\" in file_name:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def get_drone_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    drone_mode = file_name.split(\"_\")[1]\n",
        "    if drone_mode == \"A\":\n",
        "        return 0\n",
        "    elif drone_mode == \"B\":\n",
        "        return 1\n",
        "    elif drone_mode == \"C\":\n",
        "        return 2\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def get_direction_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    direction_mode = file_name.split(\"_\")[2]\n",
        "    if direction_mode == \"Back\":\n",
        "        return 0\n",
        "    elif direction_mode == \"Front\":\n",
        "        return 1\n",
        "    elif direction_mode == \"Left\":\n",
        "        return 2\n",
        "    elif direction_mode == \"Right\":\n",
        "        return 3\n",
        "    elif direction_mode == \"Clockwise\":\n",
        "        return 4\n",
        "    elif direction_mode == \"CounterClockwise\":\n",
        "        return 5\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPvZEw-SzYIB"
      },
      "source": [
        "### 2048/768/256\n",
        "Global Mean: -4.636630590752749\n",
        "Global Std: 12.173359870910645\n",
        "\n",
        "### 2048/768/192 #\n",
        "Global Mean: -4.616434057646901\n",
        "Global Std: 12.162795066833496\n",
        "\n",
        "### 2048/768/128\n",
        "Global Mean: -4.620500203780632\n",
        "Global Std: 12.1651611328125\n",
        "\n",
        "### 4096/4096/256\n",
        "Global Mean: 2.3749662920402215\n",
        "Global Std: 11.927532196044922"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fAsj90eMtYSb"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "# 2. Feature Extraction & Augmentation\n",
        "#######################\n",
        "# 계산된 전역 평균과 표준편차\n",
        "# STFT 바꿀때마다 계산해줘야함\n",
        "\n",
        "GLOBAL_MEAN =  -4.616434057646901\n",
        "GLOBAL_STD = 12.162795066833496\n",
        "def wav_to_log_stft(\n",
        "    wav_path: str,\n",
        "    sr: int,\n",
        "    n_fft: int,\n",
        "    win_length: int,\n",
        "    hop_length: int,\n",
        "    power: float,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    WAV 파일을 STFT 기반 로그 스펙트럼으로 변환.\n",
        "    - torchaudio.transforms.Spectrogram로 STFT 계산\n",
        "    - AmplitudeToDB로 로그 변환 후 global standard scaling 적용 **\n",
        "    \"\"\"\n",
        "    stft_transform = torchaudio.transforms.Spectrogram(\n",
        "        n_fft=n_fft,\n",
        "        win_length=win_length,\n",
        "        hop_length=hop_length,\n",
        "        power=power\n",
        "    )\n",
        "    wav_data, _ = torchaudio.load(wav_path)\n",
        "    spec = stft_transform(wav_data)\n",
        "    amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "    log_spec = amp_to_db(spec)\n",
        "\n",
        "    # 전체 데이터셋의 평균과 표준편차로 정규화 적용\n",
        "    # clamp 추가\n",
        "    log_spec = (log_spec - GLOBAL_MEAN) / (GLOBAL_STD + 1e-9)\n",
        "    return log_spec\n",
        "\n",
        "\n",
        "import torchaudio.transforms as T\n",
        "# -------------------------------\n",
        "# Feature Extraction & Augmentation\n",
        "# -------------------------------\n",
        "def augment_spec(spec: torch.Tensor) -> torch.Tensor:\n",
        "    max_shift = int(spec.shape[-1] * 0.1)\n",
        "    shift = torch.randint(-max_shift, max_shift + 1, (1,)).item()\n",
        "    spec = torch.roll(spec, shifts=shift, dims=-1)\n",
        "    time_mask_param = max(1, int(spec.shape[-1] * 0.05))\n",
        "    time_mask = T.TimeMasking(time_mask_param=time_mask_param)\n",
        "    spec = time_mask(spec)\n",
        "    freq_mask_param = max(1, int(spec.shape[-2] * 0.05))\n",
        "    freq_mask = T.FrequencyMasking(freq_mask_param=freq_mask_param)\n",
        "    spec = freq_mask(spec)\n",
        "    return spec\n",
        "\n",
        "#######################\n",
        "# 3. Dataset\n",
        "#######################\n",
        "class BaselineDataLoader(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_list: List[str],\n",
        "        sr: int,\n",
        "        n_fft: int,\n",
        "        win_length: int,\n",
        "        hop_length: int,\n",
        "        power: float,\n",
        "        augment: bool = False\n",
        "    ) -> None:\n",
        "        self.file_list = file_list\n",
        "        self.sr = sr\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, int, int]:\n",
        "        wav_path = self.file_list[idx]\n",
        "        spec = wav_to_log_stft(wav_path, self.sr, self.n_fft, self.win_length, self.hop_length, self.power)\n",
        "        if self.augment:\n",
        "            spec = augment_spec(spec)\n",
        "        if torch.isnan(spec).any() or torch.isinf(spec).any():\n",
        "            print(f\"Warning: Spec from {wav_path} contains NaN or Inf\")\n",
        "        anomaly_label = get_anomaly_label(wav_path)\n",
        "        drone_label = get_drone_label(wav_path)\n",
        "        direction_label = get_direction_label(wav_path)\n",
        "        # 데이터셋에서 스펙트로그램이 로드된 후 검사\n",
        "\n",
        "        return spec, anomaly_label, drone_label, direction_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MAatqFKi_-BY"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Drone Anomaly Detection Training Script\n",
        "\n",
        "이 스크립트는 향상된 Denoising Autoencoder(DAE)를 이용하여 드론 이상 탐지 모델을 학습하고 평가합니다.\n",
        "훈련 시에는 warm-up epoch 동안 L1 손실을 사용한 뒤, MSSSIM + L1 손실을 사용하고, 최종 평가 시에는 재구성 품질을 GMSD로 측정합니다.\n",
        "CosineAnnealingLR을 적용하여 학습률을 조정하며, 각 에포크마다 업데이트된 학습률을 출력합니다.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_msssim import ms_ssim\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# -------------------------------\n",
        "# Utility Functions & Set Seed\n",
        "# -------------------------------\n",
        "def set_seed(seed: int) -> None:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def match_size(source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    src_h, src_w = source.size(2), source.size(3)\n",
        "    tgt_h, tgt_w = target.size(2), target.size(3)\n",
        "    if src_h > tgt_h or src_w > tgt_w:\n",
        "        start_h = (src_h - tgt_h) // 2\n",
        "        start_w = (src_w - tgt_w) // 2\n",
        "        source = source[:, :, start_h:start_h+tgt_h, start_w:start_w+tgt_w]\n",
        "    elif src_h < tgt_h or src_w < tgt_w:\n",
        "        diff_h = tgt_h - src_h\n",
        "        diff_w = tgt_w - src_w\n",
        "        source = F.pad(source, (diff_w // 2, diff_w - diff_w // 2,\n",
        "                                diff_h // 2, diff_h - diff_h // 2))\n",
        "    return source\n",
        "\n",
        "# -------------------------------\n",
        "# Loss Functions\n",
        "# -------------------------------\n",
        "class MSSSIMLoss(nn.Module):\n",
        "    def __init__(self, window_size=7, size_average=True, data_range=1.0, K=(0.01, 0.03)):\n",
        "        super(MSSSIMLoss, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.data_range = data_range\n",
        "        self.K = K\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        return 1 - ms_ssim(img1, img2,\n",
        "                           data_range=self.data_range,\n",
        "                           win_size=self.window_size,\n",
        "                           size_average=self.size_average,\n",
        "                           K=self.K)\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, window_size=7, size_average=True, msssim_weight=0.8, l1_weight=0.2):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.msssim_loss = MSSSIMLoss(window_size, size_average)\n",
        "        self.msssim_weight = msssim_weight\n",
        "        self.l1_weight = l1_weight\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        loss_msssim = self.msssim_loss(img1, img2)\n",
        "        loss_l1 = F.l1_loss(img1, img2)\n",
        "        return self.msssim_weight * loss_msssim + self.l1_weight * loss_l1\n",
        "\n",
        "class GMSDLoss(nn.Module):\n",
        "    def __init__(self, T: float = 0.0026):\n",
        "        super(GMSDLoss, self).__init__()\n",
        "        self.T = T\n",
        "        weight_x = torch.tensor([[-1, 0, 1],\n",
        "                                 [-2, 0, 2],\n",
        "                                 [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        weight_y = torch.tensor([[-1, -2, -1],\n",
        "                                 [0, 0, 0],\n",
        "                                 [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        self.register_buffer('weight_x', weight_x)\n",
        "        self.register_buffer('weight_y', weight_y)\n",
        "\n",
        "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
        "        weight_x = self.weight_x.to(dtype=img1.dtype, device=img1.device)\n",
        "        weight_y = self.weight_y.to(dtype=img1.dtype, device=img1.device)\n",
        "        grad1_x = F.conv2d(img1, weight_x, padding=1)\n",
        "        grad1_y = F.conv2d(img1, weight_y, padding=1)\n",
        "        grad2_x = F.conv2d(img2, weight_x, padding=1)\n",
        "        grad2_y = F.conv2d(img2, weight_y, padding=1)\n",
        "        grad1 = torch.sqrt(grad1_x ** 2 + grad1_y ** 2)\n",
        "        grad2 = torch.sqrt(grad2_x ** 2 + grad2_y ** 2)\n",
        "        eps = 1e-5\n",
        "        denom = grad1 ** 2 + grad2 ** 2 + self.T\n",
        "        denom = torch.clamp(denom, min=eps)\n",
        "        gms = (2 * grad1 * grad2 + self.T) / denom\n",
        "        gmsd = torch.std(gms, unbiased=False)\n",
        "        return gmsd\n",
        "\n",
        "class MultiScaleGMSDLoss(nn.Module):\n",
        "    def __init__(self, scales=4, T: float = 0.0026):\n",
        "        super(MultiScaleGMSDLoss, self).__init__()\n",
        "        self.scales = scales\n",
        "        self.gmsd_loss = GMSDLoss(T=T)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        gmsd_vals = []\n",
        "        for scale in range(self.scales):\n",
        "            gmsd_val = self.gmsd_loss(img1, img2)\n",
        "            gmsd_vals.append(gmsd_val)\n",
        "            if scale < self.scales - 1:\n",
        "                img1 = F.interpolate(img1, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "                img2 = F.interpolate(img2, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        return sum(gmsd_vals) / len(gmsd_vals)\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset Class\n",
        "# -------------------------------\n",
        "class BaselineDataLoader(Dataset):\n",
        "    def __init__(self, file_list: list, sr: int, n_fft: int, win_length: int, hop_length: int, power: float, augment: bool = False) -> None:\n",
        "        self.file_list = file_list\n",
        "        self.sr = sr\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple:\n",
        "        wav_path = self.file_list[idx]\n",
        "        spec = wav_to_log_stft(wav_path, self.sr, self.n_fft, self.win_length, self.hop_length, self.power)\n",
        "        if self.augment:\n",
        "            spec = augment_spec(spec)\n",
        "        anomaly_label = get_anomaly_label(wav_path)\n",
        "        drone_label = get_drone_label(wav_path)\n",
        "        direction_label = get_direction_label(wav_path)\n",
        "        return spec, anomaly_label, drone_label, direction_label\n",
        "\n",
        "# -------------------------------\n",
        "# DataLoader 생성 함수\n",
        "# -------------------------------\n",
        "def get_train_val_loader(args: argparse.Namespace, pin_memory: bool = False, num_workers: int = 0):\n",
        "    file_list = sorted(os.listdir(args.train_dir))\n",
        "    file_list = [os.path.join(args.train_dir, f) for f in file_list]\n",
        "    # train_dir에는 정상 데이터만 존재한다고 가정.\n",
        "    train_files, val_files = train_test_split(file_list, test_size=0.2, random_state=2025)\n",
        "    train_files = [f for f in train_files if get_anomaly_label(f) == 0]\n",
        "    train_dataset = BaselineDataLoader(train_files, sr=args.sr, n_fft=args.n_fft,\n",
        "                                        win_length=args.win_length, hop_length=args.hop_length,\n",
        "                                        power=args.power, augment=True)\n",
        "    val_dataset = BaselineDataLoader(val_files, sr=args.sr, n_fft=args.n_fft,\n",
        "                                      win_length=args.win_length, hop_length=args.hop_length,\n",
        "                                      power=args.power, augment=False)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                              num_workers=num_workers, pin_memory=pin_memory)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def get_eval_loader(args: argparse.Namespace, pin_memory: bool = False, num_workers: int = 0):\n",
        "    file_list = sorted(os.listdir(args.eval_dir))\n",
        "    file_list = [os.path.join(args.eval_dir, f) for f in file_list]\n",
        "    dataset = BaselineDataLoader(file_list, sr=args.sr, n_fft=args.n_fft, win_length=args.win_length,\n",
        "                                 hop_length=args.hop_length, power=args.power, augment=False)\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=False,\n",
        "                        num_workers=num_workers, pin_memory=pin_memory)\n",
        "    return loader, file_list\n",
        "\n",
        "# -------------------------------\n",
        "# Model Architecture (Enhanced DAE)\n",
        "# -------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# -------------------------------\n",
        "# Residual Block\n",
        "# -------------------------------\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dropout=0.03):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# -------------------------------\n",
        "# Encoder / Decoder Blocks\n",
        "# -------------------------------\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.03):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.resblock = ResidualBlock(in_channels, out_channels, stride=2, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resblock(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, skip_channels, out_channels, dropout=0.03):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3,\n",
        "                                         stride=2, padding=1, output_padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "        total_in = out_channels + skip_channels\n",
        "        self.resblock = ResidualBlock(total_in, out_channels, stride=1, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, skips):\n",
        "        x = self.deconv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if isinstance(skips, (list, tuple)):\n",
        "            skips = [match_size(s, x) for s in skips]\n",
        "            skip = torch.cat(skips, dim=1)\n",
        "        else:\n",
        "            skip = match_size(skips, x)\n",
        "\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.resblock(x)\n",
        "        return x\n",
        "\n",
        "# -------------------------------\n",
        "# Enhanced Denoising Autoencoder\n",
        "# -------------------------------\n",
        "class EnhancedDenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self, input_channels=1, dropout=0.03):\n",
        "        super(EnhancedDenoisingAutoencoder, self).__init__()\n",
        "        self.enc1 = EncoderBlock(input_channels, 32, dropout)\n",
        "        self.enc2 = EncoderBlock(32, 64, dropout)\n",
        "        self.enc3 = EncoderBlock(64, 128, dropout)\n",
        "        self.enc4 = EncoderBlock(128, 256, dropout)\n",
        "        self.enc5 = EncoderBlock(256, 512, dropout)\n",
        "\n",
        "        # 정확한 skip channel 수로 수정\n",
        "        self.dec5 = DecoderBlock(in_channels=512, skip_channels=256, out_channels=256, dropout=dropout)\n",
        "        self.dec4 = DecoderBlock(in_channels=256, skip_channels=384, out_channels=128, dropout=dropout)\n",
        "        self.dec3 = DecoderBlock(in_channels=128, skip_channels=192, out_channels=64, dropout=dropout)\n",
        "        self.dec2 = DecoderBlock(in_channels=64,  skip_channels=96,  out_channels=32, dropout=dropout)\n",
        "\n",
        "        self.dec1 = nn.ConvTranspose2d(32, input_channels, kernel_size=3, stride=2,\n",
        "                                       padding=1, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)   # (B, 32)\n",
        "        e2 = self.enc2(e1)  # (B, 64)\n",
        "        e3 = self.enc3(e2)  # (B, 128)\n",
        "        e4 = self.enc4(e3)  # (B, 256)\n",
        "        e5 = self.enc5(e4)  # (B, 512)\n",
        "\n",
        "        d5 = self.dec5(e5, e4)              # decoder5 ← e4\n",
        "        d4 = self.dec4(d5, [e4, e3])        # decoder4 ← e4, e3\n",
        "        d3 = self.dec3(d4, [e3, e2])        # decoder3 ← e3, e2\n",
        "        d2 = self.dec2(d3, [e2, e1])        # decoder2 ← e2, e1\n",
        "        d1 = self.dec1(d2)                  # 마지막 복원\n",
        "\n",
        "        d1 = F.interpolate(d1, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
        "        return d1\n",
        "\n",
        "def DAEModel(dropout) -> nn.Module:\n",
        "    return EnhancedDenoisingAutoencoder(input_channels=1, dropout=dropout)\n",
        "\n",
        "# -------------------------------\n",
        "# Training & Evaluation Functions\n",
        "# -------------------------------\n",
        "def train_and_evaluate(args: argparse.Namespace) -> None:\n",
        "    print(\"Training started...\")\n",
        "    os.makedirs(args.result_dir, exist_ok=True)\n",
        "    os.makedirs(args.model_dir, exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    set_seed(2025)\n",
        "    if device.type == \"cuda\":\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    model = DAEModel(args.dropout).to(device)\n",
        "\n",
        "    train_loader, val_loader = get_train_val_loader(args, pin_memory=True, num_workers=args.n_workers)\n",
        "    eval_loader, eval_file_list = get_eval_loader(args, pin_memory=True, num_workers=args.n_workers)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
        "\n",
        "    # Combined loss (MSSSIM + L1) for normal training after warm-up\n",
        "    criterion = CombinedLoss(window_size=7, size_average=True, msssim_weight=0.9, l1_weight=0.1)\n",
        "    # L1 loss only for warm-up phase\n",
        "    criterion_l1 = nn.L1Loss()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        count = 0\n",
        "        p_bar = tqdm(train_loader, total=len(train_loader),\n",
        "                     desc=f\"Epoch {epoch}/{args.epochs}\", ncols=100)\n",
        "        for data in p_bar:\n",
        "            spec = data[0].to(device)\n",
        "            # 노이즈 추가 (training augmentation)\n",
        "            noisy_spec = spec + args.noise_factor * torch.randn_like(spec)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                decoded = model(noisy_spec)\n",
        "                # Warm-up: 초기 args.warmup_epochs 에서는 L1 loss만 사용\n",
        "                if epoch <= args.warmup_epochs:\n",
        "                    loss = criterion_l1(decoded, spec)\n",
        "                else:\n",
        "                    loss = criterion(decoded, spec)\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            total_loss += loss.item()\n",
        "            count += 1\n",
        "            p_bar.set_description(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = total_loss / count\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"Epoch {epoch} average train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        val_count = 0\n",
        "        y_true_val = []\n",
        "        y_pred_val = []\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                spec, anomaly_label, _, _ = val_data\n",
        "                spec = spec.to(device)\n",
        "                decoded = model(spec)\n",
        "                loss = criterion(decoded, spec)\n",
        "                total_val_loss += loss.item()\n",
        "                val_count += 1\n",
        "                y_true_val.append(anomaly_label.item())\n",
        "                y_pred_val.append(loss.item())\n",
        "        avg_val_loss = total_val_loss / val_count\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Epoch {epoch} average val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            checkpoint_path = os.path.join(args.model_dir, args.model_path)\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Epoch {epoch}: Model improved. Saved checkpoint to {checkpoint_path}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= args.early_stopping_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # 학습 곡선 저장\n",
        "    epochs_range = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Combined Loss (MSSSIM + L1)')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    curve_path = os.path.join(args.result_dir, \"learning_curve.png\")\n",
        "    plt.savefig(curve_path)\n",
        "    plt.show()\n",
        "    print(f\"Learning curve saved to {curve_path}\")\n",
        "\n",
        "    # GMSD 기반 평가 (clamp 제거)\n",
        "    gmsd_criterion = MultiScaleGMSDLoss().to(device)\n",
        "    y_true_eval = []\n",
        "    y_pred_eval = []\n",
        "    score_list = [[\"File Name\", \"True Label\", \"Anomaly Score\"]]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, (spec, anomaly_label, drone_label, _) in enumerate(tqdm(eval_loader, desc=\"Evaluating\")):\n",
        "            spec = spec.to(device)\n",
        "            decoded = model(spec)\n",
        "            loss_gmsd = gmsd_criterion(decoded, spec)\n",
        "\n",
        "            y_true_eval.append(anomaly_label.item())\n",
        "            y_pred_eval.append(loss_gmsd.item())\n",
        "\n",
        "            file_name = os.path.splitext(os.path.basename(eval_file_list[idx]))[0]\n",
        "            score_list.append([file_name, anomaly_label.item(), loss_gmsd.item()])\n",
        "\n",
        "    roc_auc_eval = metrics.roc_auc_score(np.array(y_true_eval), np.array(y_pred_eval))\n",
        "    print(f\"Final ROC AUC on eval set: {roc_auc_eval:.4f}\")\n",
        "\n",
        "    csv_path = os.path.join(args.result_dir, \"eval_score_enhanced_dae_11.csv\")\n",
        "    with open(csv_path, mode=\"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(score_list)\n",
        "    print(f\"Saved evaluation scores to {csv_path}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Argument Parsing 함수\n",
        "# -------------------------------\n",
        "def get_args() -> argparse.Namespace:\n",
        "    param = {\n",
        "        \"train_dir\": \"/content/ICSV31AIChallengeDataset/train\",\n",
        "        \"eval_dir\": \"/content/ICSV31AIChallengeDataset/eval\",\n",
        "        \"result_dir\": \"/content/drive/MyDrive\",\n",
        "        \"model_dir\": \"/content/drive/MyDrive\",\n",
        "        \"model_path\": \"enhanced_model_dae_11.pth\",\n",
        "        \"epochs\": 250,\n",
        "        \"batch_size\": 32,\n",
        "        \"lr\": 0.001,\n",
        "        \"gpu\": 0,\n",
        "        \"n_workers\": 1,\n",
        "        \"early_stopping_patience\": 20,\n",
        "        \"warmup_epochs\": 15,           # Warm-up epoch: 초기 epoch은 L1 loss만 사용\n",
        "        \"noise_factor\": 0.05,\n",
        "        \"dropout\": 0.05,\n",
        "        \"sr\": 16000,\n",
        "        \"n_fft\": 2048,\n",
        "        \"win_length\": 768,\n",
        "        \"hop_length\": 192,\n",
        "        \"power\": 2.0\n",
        "    }\n",
        "    parser = argparse.ArgumentParser()\n",
        "    for key, value in param.items():\n",
        "        parser.add_argument(f\"--{key.replace('_','-')}\", default=value, type=type(value))\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    return args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1wtKbxZPpTI"
      },
      "source": [
        "## 모델 학습 및 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05P3jl9997xr",
        "outputId": "350572d0-d5f9-4bc0-c7fe-18fbb7a6766f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Loss: 0.3627: 100%|██████████████████████████████████████| 135/135 [01:06<00:00,  2.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 average train loss: 0.4138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 average val loss: 0.6578\n",
            "Epoch 1: Model improved. Saved checkpoint to /content/drive/MyDrive/enhanced_model_dae_11.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Loss: 0.3535:  32%|████████████▍                          | 43/135 [00:19<00:40,  2.26it/s]"
          ]
        }
      ],
      "source": [
        " if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
        "    set_seed(2025)\n",
        "    train_and_evaluate(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXv2KSvVVRFd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}