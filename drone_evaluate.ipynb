{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OverfitSurvivor/code/blob/main/drone_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZCNbj1_34YHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpPxEpQ3b--R",
        "outputId": "80eb684f-be7a-45d8-e5f1-b102773140f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FaVeakiafwSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449c248d-9718-4dcb-945a-065a2eee8890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 해제 완료: /content/ICSV31AIChallengeDataset\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/ICSV31AIChallengeDataset.zip\"  # 업로드한 ZIP 파일 경로\n",
        "extract_path = \"/content/ICSV31AIChallengeDataset\"  # 압축을 풀 폴더 경로\n",
        "\n",
        "# 폴더가 없으면 생성\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# 압축 해제\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"압축 해제 완료:\", extract_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-K2Pl0PbbQ"
      },
      "source": [
        "## 모듈 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torch.pyc\n",
        "!_pycache__/torch.cpython-*.pyc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu_17WR064eG",
        "outputId": "32eda491-55b7-46f6-edc0-054ac225b4f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: torch.pyc: command not found\n",
            "/bin/bash: line 1: _pycache__/torch.cpython-*.pyc: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AcxPt6b6AAZy"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import argparse\n",
        "import os\n",
        "from typing import Any, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "421dhcInPX1n"
      },
      "source": [
        "## 라벨링 및 데이터셋 로더\n",
        "+ global scaling STFT 적용"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# 1. Utils\n",
        "#######################\n",
        "def read_csv(file_path: str) -> List:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        return list(reader)\n",
        "\n",
        "def save_csv(save_data: List[Any], save_file_path: str) -> None:\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "def get_anomaly_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    train_mode = file_name.split(\"_\")[0]\n",
        "    if train_mode == \"test\":\n",
        "        return -1\n",
        "    elif \"normal\" in file_name:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def get_drone_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    drone_mode = file_name.split(\"_\")[1]\n",
        "    if drone_mode == \"A\":\n",
        "        return 0\n",
        "    elif drone_mode == \"B\":\n",
        "        return 1\n",
        "    elif drone_mode == \"C\":\n",
        "        return 2\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def get_direction_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    direction_mode = file_name.split(\"_\")[2]\n",
        "    if direction_mode == \"Back\":\n",
        "        return 0\n",
        "    elif direction_mode == \"Front\":\n",
        "        return 1\n",
        "    elif direction_mode == \"Left\":\n",
        "        return 2\n",
        "    elif direction_mode == \"Right\":\n",
        "        return 3\n",
        "    elif direction_mode == \"Clockwise\":\n",
        "        return 4\n",
        "    elif direction_mode == \"CounterClockwise\":\n",
        "        return 5\n",
        "    else:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "bHs-4FinYIFu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global Mean: -4.636630590752749\n",
        "\n",
        "Global Std: 12.173359870910645\n",
        "\n",
        "### 2048/768/192\n",
        "Global Mean: -4.616434057646901\n",
        "Global Std: 12.162795066833496"
      ],
      "metadata": {
        "id": "SPvZEw-SzYIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# 2. Feature Extraction & Augmentation\n",
        "#######################\n",
        "# 계산된 전역 평균과 표준편차\n",
        "# STFT 바꿀때마다 계산해줘야함\n",
        "\n",
        "GLOBAL_MEAN = -4.616434057646901\n",
        "GLOBAL_STD = 12.162795066833496\n",
        "def wav_to_log_stft(\n",
        "    wav_path: str,\n",
        "    sr: int,\n",
        "    n_fft: int,\n",
        "    win_length: int,\n",
        "    hop_length: int,\n",
        "    power: float,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    WAV 파일을 STFT 기반 로그 스펙트럼으로 변환.\n",
        "    - torchaudio.transforms.Spectrogram로 STFT 계산\n",
        "    - AmplitudeToDB로 로그 변환 후 global standard scaling 적용 **\n",
        "    \"\"\"\n",
        "    stft_transform = torchaudio.transforms.Spectrogram(\n",
        "        n_fft=n_fft,\n",
        "        win_length=win_length,\n",
        "        hop_length=hop_length,\n",
        "        power=power\n",
        "    )\n",
        "    wav_data, _ = torchaudio.load(wav_path)\n",
        "    spec = stft_transform(wav_data)\n",
        "    amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "    log_spec = amp_to_db(spec)\n",
        "\n",
        "    # 전체 데이터셋의 평균과 표준편차로 정규화 적용\n",
        "    log_spec = (log_spec - GLOBAL_MEAN) / (GLOBAL_STD + 1e-9)\n",
        "    return log_spec\n",
        "\n",
        "\n",
        "def augment_spec(spec: torch.Tensor) -> torch.Tensor:\n",
        "    max_shift = int(spec.shape[-1] * 0.1)\n",
        "    shift = torch.randint(-max_shift, max_shift + 1, (1,)).item()\n",
        "    spec = torch.roll(spec, shifts=shift, dims=-1)\n",
        "    time_mask_param = max(1, int(spec.shape[-1] * 0.05))\n",
        "    time_mask = torchaudio.transforms.TimeMasking(time_mask_param=time_mask_param)\n",
        "    spec = time_mask(spec)\n",
        "    freq_mask_param = max(1, int(spec.shape[-2] * 0.05))\n",
        "    freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask_param)\n",
        "    spec = freq_mask(spec)\n",
        "    return spec\n",
        "\n",
        "#######################\n",
        "# 3. Dataset\n",
        "#######################\n",
        "class BaselineDataLoader(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_list: List[str],\n",
        "        sr: int,\n",
        "        n_fft: int,\n",
        "        win_length: int,\n",
        "        hop_length: int,\n",
        "        power: float,\n",
        "        augment: bool = False\n",
        "    ) -> None:\n",
        "        self.file_list = file_list\n",
        "        self.sr = sr\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, int, int]:\n",
        "        wav_path = self.file_list[idx]\n",
        "        spec = wav_to_log_stft(wav_path, self.sr, self.n_fft, self.win_length, self.hop_length, self.power)\n",
        "        if self.augment:\n",
        "            spec = augment_spec(spec)\n",
        "        anomaly_label = get_anomaly_label(wav_path)\n",
        "        drone_label = get_drone_label(wav_path)\n",
        "        direction_label = get_direction_label(wav_path)\n",
        "        return spec, anomaly_label, drone_label, direction_label"
      ],
      "metadata": {
        "id": "fAsj90eMtYSb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1wtKbxZPpTI"
      },
      "source": [
        "## 모델 학습 및 평가"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-msssim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRxoDW1xD44V",
        "outputId": "adbac8f0-8082-4f97-e9d0-8c6917bddd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-msssim\n",
            "  Using cached pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch-msssim) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch-msssim) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch-msssim) (3.0.2)\n",
            "Using cached pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/127.9 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA8-fOjQPRUV"
      },
      "source": [
        "## 평가 데이터 바꿔서 모델 가중치 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 기본 GMSD Loss 구현\n",
        "# ----------------------------------------------------------------\n",
        "class GMSDLoss(nn.Module):\n",
        "    def __init__(self, T: float = 0.0026):\n",
        "        super(GMSDLoss, self).__init__()\n",
        "        self.T = T\n",
        "        weight_x = torch.tensor([[-1, 0, 1],\n",
        "                                 [-2, 0, 2],\n",
        "                                 [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        weight_y = torch.tensor([[-1, -2, -1],\n",
        "                                 [0, 0, 0],\n",
        "                                 [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        self.register_buffer('weight_x', weight_x)\n",
        "        self.register_buffer('weight_y', weight_y)\n",
        "\n",
        "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
        "        weight_x = self.weight_x.to(dtype=img1.dtype, device=img1.device)\n",
        "        weight_y = self.weight_y.to(dtype=img1.dtype, device=img1.device)\n",
        "        grad1_x = F.conv2d(img1, weight_x, padding=1)\n",
        "        grad1_y = F.conv2d(img1, weight_y, padding=1)\n",
        "        grad2_x = F.conv2d(img2, weight_x, padding=1)\n",
        "        grad2_y = F.conv2d(img2, weight_y, padding=1)\n",
        "        grad1 = torch.sqrt(grad1_x ** 2 + grad1_y ** 2)\n",
        "        grad2 = torch.sqrt(grad2_x ** 2 + grad2_y ** 2)\n",
        "        eps = 1e-5\n",
        "        denom = grad1 ** 2 + grad2 ** 2 + self.T\n",
        "        denom = torch.clamp(denom, min=eps)\n",
        "        gms = (2 * grad1 * grad2 + self.T) / denom\n",
        "        gmsd = torch.std(gms, unbiased=False)\n",
        "        return gmsd\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Multi-Scale GMSD Loss (MS-GMSD)\n",
        "# ----------------------------------------------------------------\n",
        "class MultiScaleGMSDLoss(nn.Module):\n",
        "    def __init__(self, scales=4, T: float = 0.0026):\n",
        "        super(MultiScaleGMSDLoss, self).__init__()\n",
        "        self.scales = scales\n",
        "        self.gmsd_loss = GMSDLoss(T=T)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        gmsd_vals = []\n",
        "        for scale in range(self.scales):\n",
        "            gmsd_val = self.gmsd_loss(img1, img2)\n",
        "            gmsd_vals.append(gmsd_val)\n",
        "            if scale < self.scales - 1:\n",
        "                img1 = F.interpolate(img1, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "                img2 = F.interpolate(img2, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        return sum(gmsd_vals) / len(gmsd_vals)"
      ],
      "metadata": {
        "id": "P6yyvG1UO6eC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RHAscWQSkqFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4dd0c6c-4009-4c84-cea6-2ea301f78f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMSD+L1: ROC AUC = 0.8550\n",
            "MS-GMSD+L1: ROC AUC = 0.8563\n",
            "저장 완료: /content/drive/MyDrive/anomaly_scores_best_MS-GMSD+L1.csv\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# 환경 설정 및 파라미터 정의\n",
        "# ---------------------------\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn import metrics\n",
        "\n",
        "class Args:\n",
        "    eval_dir = \"/content/ICSV31AIChallengeDataset/eval\"\n",
        "    result_dir = \"/content/drive/MyDrive\"\n",
        "    model_dir = \"/content/drive/MyDrive\"\n",
        "    model_path = \"/content/model_dae_0423.pth\"\n",
        "    batch_size = 1\n",
        "    gpu = 0\n",
        "    n_workers = 1\n",
        "    sr = 16000\n",
        "    n_fft = 2048\n",
        "    win_length = 768\n",
        "    hop_length = 192\n",
        "    power = 2.0\n",
        "    dropout = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# ---------------------------\n",
        "# 기본 함수 및 유틸\n",
        "# ---------------------------\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def get_anomaly_label(wav_path):\n",
        "    return 0 if \"normal\" in os.path.basename(wav_path).lower() else 1\n",
        "\n",
        "def match_size(source, target):\n",
        "    src_h, src_w = source.size(2), source.size(3)\n",
        "    tgt_h, tgt_w = target.size(2), target.size(3)\n",
        "    if src_h > tgt_h or src_w > tgt_w:\n",
        "        start_h = (src_h - tgt_h) // 2\n",
        "        start_w = (src_w - tgt_w) // 2\n",
        "        source = source[:, :, start_h:start_h+tgt_h, start_w:start_w+tgt_w]\n",
        "    elif src_h < tgt_h or src_w < tgt_w:\n",
        "        diff_h, diff_w = tgt_h - src_h, tgt_w - src_w\n",
        "        source = F.pad(source, (diff_w // 2, diff_w - diff_w // 2,\n",
        "                                diff_h // 2, diff_h - diff_h // 2))\n",
        "    return source\n",
        "\n",
        "# ---------------------------\n",
        "# 손실 함수 구현\n",
        "# ---------------------------\n",
        "\n",
        "class GMSDLoss(nn.Module):\n",
        "    def __init__(self, T: float = 0.0026):\n",
        "        super().__init__()\n",
        "        weight_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        weight_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
        "        self.register_buffer('weight_x', weight_x)\n",
        "        self.register_buffer('weight_y', weight_y)\n",
        "        self.T = T\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        grad1_x = F.conv2d(img1, self.weight_x, padding=1)\n",
        "        grad1_y = F.conv2d(img1, self.weight_y, padding=1)\n",
        "        grad2_x = F.conv2d(img2, self.weight_x, padding=1)\n",
        "        grad2_y = F.conv2d(img2, self.weight_y, padding=1)\n",
        "        grad1 = torch.sqrt(grad1_x**2 + grad1_y**2)\n",
        "        grad2 = torch.sqrt(grad2_x**2 + grad2_y**2)\n",
        "        gms = (2 * grad1 * grad2 + self.T) / (grad1**2 + grad2**2 + self.T + 1e-5)\n",
        "        return torch.std(gms, unbiased=False)\n",
        "\n",
        "class MultiScaleGMSDLoss(nn.Module):\n",
        "    def __init__(self, scales=4, T: float = 0.0026):\n",
        "        super().__init__()\n",
        "        self.gmsd = GMSDLoss(T=T)\n",
        "        self.scales = scales\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        scores = []\n",
        "        for _ in range(self.scales):\n",
        "            scores.append(self.gmsd(x, y))\n",
        "            if _ < self.scales - 1:\n",
        "                x = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "                y = F.interpolate(y, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, loss1, loss2, alpha=0.2):\n",
        "        super().__init__()\n",
        "        self.loss1 = loss1\n",
        "        self.loss2 = loss2\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.alpha * self.loss1(x, y) + (1 - self.alpha) * self.loss2(x, y)\n",
        "\n",
        "def get_loss_functions():\n",
        "    return {\n",
        "        \"GMSD+L1\": CombinedLoss(GMSDLoss(), nn.L1Loss()),\n",
        "        \"MS-GMSD+L1\": CombinedLoss(MultiScaleGMSDLoss(), nn.L1Loss())\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# 데이터셋 정의\n",
        "# ---------------------------\n",
        "\n",
        "GLOBAL_MEAN = -4.636630590752749\n",
        "GLOBAL_STD = 12.173359870910645\n",
        "\n",
        "def wav_to_log_stft(wav_path, sr, n_fft, win_length, hop_length, power):\n",
        "    wav, _ = torchaudio.load(wav_path)\n",
        "    spec = torchaudio.transforms.Spectrogram(n_fft=n_fft, win_length=win_length,\n",
        "                                             hop_length=hop_length, power=power)(wav)\n",
        "    log_spec = torchaudio.transforms.AmplitudeToDB()(spec)\n",
        "    return (log_spec - GLOBAL_MEAN) / (GLOBAL_STD + 1e-9)\n",
        "\n",
        "class BaselineDataset(Dataset):\n",
        "    def __init__(self, file_list):\n",
        "        self.file_list = file_list\n",
        "\n",
        "    def __len__(self): return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        spec = wav_to_log_stft(path, args.sr, args.n_fft, args.win_length, args.hop_length, args.power)\n",
        "        label = get_anomaly_label(path)\n",
        "        return spec, label\n",
        "\n",
        "def get_eval_loader():\n",
        "    file_list = sorted([os.path.join(args.eval_dir, f) for f in os.listdir(args.eval_dir)])\n",
        "    dataset = BaselineDataset(file_list)\n",
        "    return DataLoader(dataset, batch_size=1, shuffle=False, num_workers=args.n_workers), file_list\n",
        "\n",
        "#############################\n",
        "# Utility: 크기 맞춤 함수\n",
        "#############################\n",
        "def match_size(source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    src_h, src_w = source.size(2), source.size(3)\n",
        "    tgt_h, tgt_w = target.size(2), target.size(3)\n",
        "    if src_h > tgt_h or src_w > tgt_w:\n",
        "        start_h = (src_h - tgt_h) // 2\n",
        "        start_w = (src_w - tgt_w) // 2\n",
        "        source = source[:, :, start_h:start_h+tgt_h, start_w:start_w+tgt_w]\n",
        "    elif src_h < tgt_h or src_w < tgt_w:\n",
        "        diff_h = tgt_h - src_h\n",
        "        diff_w = tgt_w - src_w\n",
        "        source = F.pad(source, (diff_w // 2, diff_w - diff_w // 2,\n",
        "                                diff_h // 2, diff_h - diff_h // 2))\n",
        "    return source\n",
        "\n",
        "#############################\n",
        "# Model Architecture (DAE)\n",
        "#############################\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dropout=0.03):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.03):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.resblock = ResidualBlock(in_channels, out_channels, stride=2, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resblock(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.03):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3,\n",
        "                                         stride=2, padding=1, output_padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.resblock = ResidualBlock(out_channels * 2, out_channels, stride=1, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.deconv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        skip = match_size(skip, x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.resblock(x)\n",
        "        return x\n",
        "\n",
        "class DenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self, input_channels=1, dropout=0.03):\n",
        "        super(DenoisingAutoencoder, self).__init__()\n",
        "        self.enc1 = EncoderBlock(input_channels, 32, dropout)\n",
        "        self.enc2 = EncoderBlock(32, 64, dropout)\n",
        "        self.enc3 = EncoderBlock(64, 128, dropout)\n",
        "        self.enc4 = EncoderBlock(128, 256, dropout)\n",
        "        self.enc5 = EncoderBlock(256, 512, dropout)\n",
        "        self.dec5 = DecoderBlock(512, 256, dropout)\n",
        "        self.dec4 = DecoderBlock(256, 128, dropout)\n",
        "        self.dec3 = DecoderBlock(128, 64, dropout)\n",
        "        self.dec2 = DecoderBlock(64, 32, dropout)\n",
        "        self.dec1 = nn.ConvTranspose2d(32, input_channels, kernel_size=3, stride=2,\n",
        "                                       padding=1, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)   # (B, 32, H/2, W/2)\n",
        "        e2 = self.enc2(e1)  # (B, 64, H/4, W/4)\n",
        "        e3 = self.enc3(e2)  # (B, 128, H/8, W/8)\n",
        "        e4 = self.enc4(e3)  # (B, 256, H/16, W/16)\n",
        "        e5 = self.enc5(e4)  # (B, 512, H/32, W/32)\n",
        "        d5 = self.dec5(e5, e4)\n",
        "        d4 = self.dec4(d5, e3)\n",
        "        d3 = self.dec3(d4, e2)\n",
        "        d2 = self.dec2(d3, e1)\n",
        "        d1 = self.dec1(d2)\n",
        "        d1 = F.interpolate(d1, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        return d1\n",
        "\n",
        "def DAEModel(dropout) -> nn.Module:\n",
        "    return DenoisingAutoencoder(input_channels=1, dropout=dropout)\n",
        "\n",
        "# ---------------------------\n",
        "# 평가 함수\n",
        "# ---------------------------\n",
        "\n",
        "def evaluate_model_gridsearch(model, loader, loss_fns, device):\n",
        "    model.eval()\n",
        "    results, loss_outputs = {}, {}\n",
        "    for name, fn in loss_fns.items():\n",
        "        y_true, y_scores, per_file = [], [], []\n",
        "        for i, (spec, label) in enumerate(loader):\n",
        "            spec = spec.to(device)\n",
        "            with torch.no_grad():\n",
        "                out = model(spec)\n",
        "                score = fn(out, spec).item()\n",
        "            y_true.append(label.item())\n",
        "            y_scores.append(score)\n",
        "            per_file.append((label.item(), score, i))\n",
        "        auc = metrics.roc_auc_score(np.array(y_true), np.array(y_scores))\n",
        "        print(f\"{name}: ROC AUC = {auc:.4f}\")\n",
        "        results[name] = auc\n",
        "        loss_outputs[name] = per_file\n",
        "    return results, loss_outputs\n",
        "\n",
        "def save_best_scores(best_name, outputs, file_list):\n",
        "    path = os.path.join(args.result_dir, f\"anomaly_scores_best_{best_name}.csv\")\n",
        "    with open(path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"File Name\", \"True Label\", \"Score\"])\n",
        "        for label, score, idx in outputs[best_name]:\n",
        "            name = os.path.splitext(os.path.basename(file_list[idx]))[0]\n",
        "            writer.writerow([name, label, score])\n",
        "    print(\"저장 완료:\", path)\n",
        "\n",
        "# ---------------------------\n",
        "# 전체 실행\n",
        "# ---------------------------\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "set_seed(2025)\n",
        "\n",
        "model = DAEModel(dropout=args.dropout).to(device)\n",
        "state = torch.load(os.path.join(args.model_dir, args.model_path), map_location=device)\n",
        "model.load_state_dict(state)\n",
        "\n",
        "loader, file_list = get_eval_loader()\n",
        "loss_fns = get_loss_functions()\n",
        "loss_fns = {k: v.to(device) for k, v in loss_fns.items()}\n",
        "\n",
        "results, outputs = evaluate_model_gridsearch(model, loader, loss_fns, device)\n",
        "best_name = max(results, key=results.get)\n",
        "save_best_scores(best_name, outputs, file_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xrva7V9HJ30m"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}