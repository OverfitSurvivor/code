{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OverfitSurvivor/code/blob/main/drone_DAE_loss_MSSSIM_L1_good_train_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-msssim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sFS4T2PmVyc",
        "outputId": "78dd9ad6-182c-4a71-dae9-faf38e8a62c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-msssim\n",
            "  Using cached pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch-msssim) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->pytorch-msssim)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch-msssim) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch-msssim) (3.0.2)\n",
            "Using cached pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-msssim\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o918hszAlfDD",
        "outputId": "3dea757b-60ea-4afa-c1cc-cba25456aeab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaVeakiafwSP",
        "outputId": "83fa9518-5323-4051-d91f-d9b36458de63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 해제 완료: /content/ICSV31AIChallengeDataset\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/ICSV31AIChallengeDataset.zip\"  # 업로드한 ZIP 파일 경로\n",
        "extract_path = \"/content/ICSV31AIChallengeDataset\"  # 압축을 풀 폴더 경로\n",
        "\n",
        "# 폴더가 없으면 생성\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# 압축 해제\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"압축 해제 완료:\", extract_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-K2Pl0PbbQ"
      },
      "source": [
        "## 모듈 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AcxPt6b6AAZy"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import argparse\n",
        "import os\n",
        "from typing import Any, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "421dhcInPX1n"
      },
      "source": [
        "## 라벨링 및 데이터셋 로더\n",
        "+ global scaling STFT 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bHs-4FinYIFu"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "# 1. Utils\n",
        "#######################\n",
        "def read_csv(file_path: str) -> List:\n",
        "    with open(file_path, \"r\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        return list(reader)\n",
        "\n",
        "def save_csv(save_data: List[Any], save_file_path: str) -> None:\n",
        "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "        writer.writerows(save_data)\n",
        "\n",
        "def get_anomaly_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    train_mode = file_name.split(\"_\")[0]\n",
        "    if train_mode == \"test\":\n",
        "        return -1\n",
        "    elif \"normal\" in file_name:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def get_drone_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    drone_mode = file_name.split(\"_\")[1]\n",
        "    if drone_mode == \"A\":\n",
        "        return 0\n",
        "    elif drone_mode == \"B\":\n",
        "        return 1\n",
        "    elif drone_mode == \"C\":\n",
        "        return 2\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def get_direction_label(file_path: str) -> int:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    direction_mode = file_name.split(\"_\")[2]\n",
        "    if direction_mode == \"Back\":\n",
        "        return 0\n",
        "    elif direction_mode == \"Front\":\n",
        "        return 1\n",
        "    elif direction_mode == \"Left\":\n",
        "        return 2\n",
        "    elif direction_mode == \"Right\":\n",
        "        return 3\n",
        "    elif direction_mode == \"Clockwise\":\n",
        "        return 4\n",
        "    elif direction_mode == \"CounterClockwise\":\n",
        "        return 5\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPvZEw-SzYIB"
      },
      "source": [
        "Global Mean: -4.636630590752749\n",
        "Global Std: 12.173359870910645\n",
        "\n",
        "Global Mean: -4.616434057646901\n",
        "Global Std: 12.162795066833496\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MAatqFKi_-BY"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "# 2. Feature Extraction & Augmentation\n",
        "#######################\n",
        "# 계산된 전역 평균과 표준편차\n",
        "# STFT 바꿀때마다 계산해줘야함\n",
        "\n",
        "GLOBAL_MEAN = -4.616434057646901\n",
        "GLOBAL_STD = 12.162795066833496\n",
        "def wav_to_log_stft(\n",
        "    wav_path: str,\n",
        "    sr: int,\n",
        "    n_fft: int,\n",
        "    win_length: int,\n",
        "    hop_length: int,\n",
        "    power: float,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    WAV 파일을 STFT 기반 로그 스펙트럼으로 변환.\n",
        "    - torchaudio.transforms.Spectrogram로 STFT 계산\n",
        "    - AmplitudeToDB로 로그 변환 후 global standard scaling 적용 **\n",
        "    \"\"\"\n",
        "    stft_transform = torchaudio.transforms.Spectrogram(\n",
        "        n_fft=n_fft,\n",
        "        win_length=win_length,\n",
        "        hop_length=hop_length,\n",
        "        power=power\n",
        "    )\n",
        "    wav_data, _ = torchaudio.load(wav_path)\n",
        "    spec = stft_transform(wav_data)\n",
        "    amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "    log_spec = amp_to_db(spec)\n",
        "\n",
        "    # 전체 데이터셋의 평균과 표준편차로 정규화 적용\n",
        "    log_spec = (log_spec - GLOBAL_MEAN) / (GLOBAL_STD + 1e-9)\n",
        "    return log_spec\n",
        "\n",
        "\n",
        "def augment_spec(spec: torch.Tensor) -> torch.Tensor:\n",
        "    max_shift = int(spec.shape[-1] * 0.1)\n",
        "    shift = torch.randint(-max_shift, max_shift + 1, (1,)).item()\n",
        "    spec = torch.roll(spec, shifts=shift, dims=-1)\n",
        "    time_mask_param = max(1, int(spec.shape[-1] * 0.05))\n",
        "    time_mask = torchaudio.transforms.TimeMasking(time_mask_param=time_mask_param)\n",
        "    spec = time_mask(spec)\n",
        "    freq_mask_param = max(1, int(spec.shape[-2] * 0.05))\n",
        "    freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask_param)\n",
        "    spec = freq_mask(spec)\n",
        "    return spec\n",
        "\n",
        "#######################\n",
        "# 3. Dataset\n",
        "#######################\n",
        "class BaselineDataLoader(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_list: List[str],\n",
        "        sr: int,\n",
        "        n_fft: int,\n",
        "        win_length: int,\n",
        "        hop_length: int,\n",
        "        power: float,\n",
        "        augment: bool = False\n",
        "    ) -> None:\n",
        "        self.file_list = file_list\n",
        "        self.sr = sr\n",
        "        self.n_fft = n_fft\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "        self.power = power\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, int, int]:\n",
        "        wav_path = self.file_list[idx]\n",
        "        spec = wav_to_log_stft(wav_path, self.sr, self.n_fft, self.win_length, self.hop_length, self.power)\n",
        "        if self.augment:\n",
        "            spec = augment_spec(spec)\n",
        "        anomaly_label = get_anomaly_label(wav_path)\n",
        "        drone_label = get_drone_label(wav_path)\n",
        "        direction_label = get_direction_label(wav_path)\n",
        "        return spec, anomaly_label, drone_label, direction_label\n",
        "\n",
        "from typing import Tuple, List\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1wtKbxZPpTI"
      },
      "source": [
        "## 모델 학습 및 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-9ilGdRReul"
      },
      "outputs": [],
      "source": [
        "def get_train_loader(args: argparse.Namespace, pin_memory: bool = False, num_workers: int = 0) -> DataLoader:\n",
        "    file_list = sorted(os.listdir(args.train_dir))\n",
        "    file_list = [os.path.join(args.train_dir, f) for f in file_list if get_anomaly_label(f) == 0]\n",
        "    dataset = BaselineDataLoader(\n",
        "        file_list,\n",
        "        sr=args.sr,\n",
        "        n_fft=args.n_fft,\n",
        "        win_length=args.win_length,\n",
        "        hop_length=args.hop_length,\n",
        "        power=args.power,\n",
        "        augment=True\n",
        "    )\n",
        "\n",
        "    # num_workers가 0일 경우 prefetch_factor를 제거\n",
        "    if num_workers > 0:\n",
        "        return DataLoader(dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory,\n",
        "                          persistent_workers=True, prefetch_factor=2)\n",
        "    else:\n",
        "        return DataLoader(dataset, batch_size=args.batch_size, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "\n",
        "def get_eval_loader(args: argparse.Namespace, pin_memory: bool = False, num_workers: int = 0) -> Tuple[DataLoader, List[str]]:\n",
        "    file_list = sorted(os.listdir(args.eval_dir))\n",
        "    file_list = [os.path.join(args.eval_dir, f) for f in file_list]\n",
        "    dataset = BaselineDataLoader(\n",
        "        file_list,\n",
        "        sr=args.sr,\n",
        "        n_fft=args.n_fft,\n",
        "        win_length=args.win_length,\n",
        "        hop_length=args.hop_length,\n",
        "        power=args.power,\n",
        "        augment=False\n",
        "    )\n",
        "    if num_workers > 0:\n",
        "        return DataLoader(dataset, batch_size=1, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory,\n",
        "                          persistent_workers=True, prefetch_factor=2), file_list\n",
        "    else:\n",
        "        return DataLoader(dataset, batch_size=1, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory), file_list\n",
        "\n",
        "\n",
        "from pytorch_msssim import ms_ssim  # ms_ssim 함수 임포트\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "#####################################\n",
        "# MSSSIM Loss Implementation\n",
        "#####################################\n",
        "class MSSSIMLoss(nn.Module):\n",
        "    def __init__(self, window_size=11, size_average=True, data_range=1.0, K=(0.01, 0.03)):\n",
        "        \"\"\"\n",
        "        MSSSIMLoss는 pytorch_msssim의 ms_ssim 함수를 이용하여,\n",
        "        두 이미지 간의 다중 스케일 구조적 유사성을 평가합니다.\n",
        "        \"\"\"\n",
        "        super(MSSSIMLoss, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.data_range = data_range\n",
        "        self.K = K\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        # ms_ssim 값은 1에 가까울수록 유사하므로, loss는 1 - ms_ssim으로 정의\n",
        "        return 1 - ms_ssim(img1, img2,\n",
        "                           data_range=self.data_range,\n",
        "                           win_size=self.window_size,\n",
        "                           size_average=self.size_average,\n",
        "                           K=self.K)\n",
        "\n",
        "#####################################\n",
        "# Combined Loss (MSSSIM + L1)\n",
        "#####################################\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, window_size=11, size_average=True, msssim_weight=0.8, l1_weight=0.2):\n",
        "        \"\"\"\n",
        "        MSSSIM과 L1 손실을 결합합니다.\n",
        "          - window_size: MSSSIM 계산 시 윈도우 크기\n",
        "          - size_average: 손실 산출 시 평균 사용 여부\n",
        "          - msssim_weight: MSSSIM 손실의 가중치\n",
        "          - l1_weight: L1 손실의 가중치\n",
        "        \"\"\"\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.msssim_loss = MSSSIMLoss(window_size, size_average)\n",
        "        self.msssim_weight = msssim_weight\n",
        "        self.l1_weight = l1_weight\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        loss_msssim = self.msssim_loss(img1, img2)\n",
        "        loss_l1 = F.l1_loss(img1, img2)\n",
        "        return self.msssim_weight * loss_msssim + self.l1_weight * loss_l1\n",
        "\n",
        "\n",
        "#############################\n",
        "# Utility: 크기 맞춤 함수\n",
        "#############################\n",
        "def match_size(source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    src_h, src_w = source.size(2), source.size(3)\n",
        "    tgt_h, tgt_w = target.size(2), target.size(3)\n",
        "    if src_h > tgt_h or src_w > tgt_w:\n",
        "        start_h = (src_h - tgt_h) // 2\n",
        "        start_w = (src_w - tgt_w) // 2\n",
        "        source = source[:, :, start_h:start_h+tgt_h, start_w:start_w+tgt_w]\n",
        "    elif src_h < tgt_h or src_w < tgt_w:\n",
        "        diff_h = tgt_h - src_h\n",
        "        diff_w = tgt_w - src_w\n",
        "        source = F.pad(source, (diff_w // 2, diff_w - diff_w // 2,\n",
        "                                diff_h // 2, diff_h - diff_h // 2))\n",
        "    return source\n",
        "\n",
        "#############################\n",
        "# Model Architecture (DAE)\n",
        "#############################\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, dropout=0.03):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.03):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.resblock = ResidualBlock(in_channels, out_channels, stride=2, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resblock(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.03):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3,\n",
        "                                         stride=2, padding=1, output_padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.resblock = ResidualBlock(out_channels * 2, out_channels, stride=1, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.deconv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        skip = match_size(skip, x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.resblock(x)\n",
        "        return x\n",
        "\n",
        "class DenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self, input_channels=1, dropout=0.03):\n",
        "        super(DenoisingAutoencoder, self).__init__()\n",
        "        self.enc1 = EncoderBlock(input_channels, 32, dropout)\n",
        "        self.enc2 = EncoderBlock(32, 64, dropout)\n",
        "        self.enc3 = EncoderBlock(64, 128, dropout)\n",
        "        self.enc4 = EncoderBlock(128, 256, dropout)\n",
        "        self.enc5 = EncoderBlock(256, 512, dropout)\n",
        "        self.dec5 = DecoderBlock(512, 256, dropout)\n",
        "        self.dec4 = DecoderBlock(256, 128, dropout)\n",
        "        self.dec3 = DecoderBlock(128, 64, dropout)\n",
        "        self.dec2 = DecoderBlock(64, 32, dropout)\n",
        "        self.dec1 = nn.ConvTranspose2d(32, input_channels, kernel_size=3, stride=2,\n",
        "                                       padding=1, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)   # (B, 32, H/2, W/2)\n",
        "        e2 = self.enc2(e1)  # (B, 64, H/4, W/4)\n",
        "        e3 = self.enc3(e2)  # (B, 128, H/8, W/8)\n",
        "        e4 = self.enc4(e3)  # (B, 256, H/16, W/16)\n",
        "        e5 = self.enc5(e4)  # (B, 512, H/32, W/32)\n",
        "        d5 = self.dec5(e5, e4)\n",
        "        d4 = self.dec4(d5, e3)\n",
        "        d3 = self.dec3(d4, e2)\n",
        "        d2 = self.dec2(d3, e1)\n",
        "        d1 = self.dec1(d2)\n",
        "        d1 = F.interpolate(d1, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "        return d1\n",
        "\n",
        "def DAEModel(dropout) -> nn.Module:\n",
        "    return DenoisingAutoencoder(input_channels=1, dropout=dropout)\n",
        "\n",
        "#############################\n",
        "# Train & Evaluate 관련 함수들\n",
        "#############################\n",
        "def get_args() -> argparse.Namespace:\n",
        "    param = {\n",
        "        \"train_dir\": \"/content/ICSV31AIChallengeDataset/train\",\n",
        "        \"eval_dir\": \"/content/ICSV31AIChallengeDataset/eval\",\n",
        "        \"result_dir\": \"/content/drive/MyDrive\",\n",
        "        \"model_dir\": \"/content/drive/MyDrive\",\n",
        "        \"model_path\": \"model_dae_0420.pth\",\n",
        "        \"epochs\": 200,\n",
        "        \"batch_size\": 32,\n",
        "        \"lr\": 0.001,\n",
        "        \"gpu\": 0,\n",
        "        \"n_workers\": 1,\n",
        "        \"early_stopping_patience\": 15,\n",
        "        \"noise_factor\": 0.2,\n",
        "        \"dropout\": 0.05,\n",
        "        \"sr\": 16000,\n",
        "        \"n_fft\": 2048,\n",
        "        \"win_length\": 768,\n",
        "        \"hop_length\":192,\n",
        "        \"power\": 2.0\n",
        "    }\n",
        "    parser = argparse.ArgumentParser()\n",
        "    for key, value in param.items():\n",
        "        parser.add_argument(f\"--{key.replace('_','-')}\", default=value, type=type(value))\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def train_and_evaluate(args: argparse.Namespace) -> None:\n",
        "    print(\"Training started...\")\n",
        "    os.makedirs(args.result_dir, exist_ok=True)\n",
        "    os.makedirs(args.model_dir, exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    set_seed(2025)\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    model = DAEModel(args.dropout).to(device)\n",
        "\n",
        "    try:\n",
        "        from torchsummary import summary\n",
        "        summary(model, input_size=(1, 256, 256))\n",
        "    except ImportError:\n",
        "        print(model)\n",
        "\n",
        "    train_loader = get_train_loader(args, pin_memory=True, num_workers=args.n_workers)\n",
        "    test_loader, eval_file_list = get_eval_loader(args, pin_memory=True, num_workers=args.n_workers)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
        "    criterion = CombinedLoss(window_size=7, size_average=True, msssim_weight=0.8, l1_weight=0.2)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_losses = []\n",
        "    best_train_loss = float('inf')   # << 초기 최고 train loss\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        count = 0\n",
        "        p_bar = tqdm(train_loader, total=len(train_loader),\n",
        "                     desc=f\"Epoch {epoch}/{args.epochs}\", ncols=100)\n",
        "        for data in p_bar:\n",
        "            spec = data[0].to(device)\n",
        "            noisy_spec = spec + args.noise_factor * torch.randn_like(spec)\n",
        "            optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                decoded = model(noisy_spec)\n",
        "                loss = criterion(decoded, spec)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            count += 1\n",
        "            p_bar.set_description(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = total_loss / count\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"Epoch {epoch} average train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # train loss가 개선되면 모델 저장\n",
        "        if avg_train_loss < best_train_loss:\n",
        "            best_train_loss = avg_train_loss\n",
        "            checkpoint_path = os.path.join(args.model_dir, args.model_path)\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"  ⇒ Improved train loss; saved model to {checkpoint_path}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # 학습 곡선 그리기 및 저장\n",
        "    epochs_range = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Combined Loss (SSIM + L1)')\n",
        "    plt.title('Training Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    curve_path = os.path.join(args.result_dir, \"training_curve.png\")\n",
        "    plt.savefig(curve_path)\n",
        "    plt.show()\n",
        "    print(f\"Training curve saved to {curve_path}\")\n",
        "\n",
        "    # 최종 평가: eval 데이터로 테스트\n",
        "    print(\"Testing on evaluation data...\")\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for spec, anomaly_label, _, _ in test_loader:\n",
        "            spec = spec.to(device)\n",
        "            decoded = model(spec)\n",
        "            loss = criterion(decoded, spec).item()\n",
        "            y_pred.append(loss)\n",
        "            y_true.append(anomaly_label.item() if anomaly_label.dim() == 0 else anomaly_label.cpu().numpy()[0])\n",
        "\n",
        "    pr_auc = metrics.average_precision_score(y_true, y_pred)\n",
        "    print(f\"Evaluation AUC: {pr_auc:.4f}\")\n",
        "\n",
        "    # CSV 저장\n",
        "    import csv\n",
        "    csv_path = os.path.join(args.result_dir, \"eval_score_0418.csv\")\n",
        "    with open(csv_path, mode=\"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"File\", \"Score\"])\n",
        "        for idx, loss in enumerate(y_pred):\n",
        "            file_name = os.path.splitext(os.path.basename(eval_file_list[idx]))[0]\n",
        "            writer.writerow([file_name, loss])\n",
        "    print(f\"Saved evaluation scores to {csv_path}\")\n",
        "\n",
        "    # 드론 타입별 PR AUC 계산\n",
        "    drone_type_list = [\"A\", \"B\", \"C\"]\n",
        "    for drone_type in drone_type_list:\n",
        "        indices = [i for i, label in enumerate(drone_label_list) if label == drone_type_list.index(drone_type)]\n",
        "        if not indices:\n",
        "            print(f\"Drone type {drone_type}: no samples\")\n",
        "            continue\n",
        "        pred = [y_pred[i] for i in indices]\n",
        "        true = [y_true[i] for i in indices]\n",
        "        auc = metrics.average_precision_score(true, pred)\n",
        "        print(f\"Drone type {drone_type} AUC: {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05P3jl9997xr"
      },
      "outputs": [],
      "source": [
        " if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
        "    set_seed(2025)\n",
        "    train_and_evaluate(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "muHlv3xKzXol"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}